{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "1         [71691, 90171, 122032, 209778, 821396, 877468,...\n",
       "2         [13069, 15558, 22013, 39714, 63441, 96192, 155...\n",
       "3         [66009, 72533, 392052, 593522, 599128, 661347,...\n",
       "4         [31070, 39588, 72989, 215041, 574787, 589886, ...\n",
       "5         [24333, 43235, 75011, 296145, 350759, 795508, ...\n",
       "                                ...                        \n",
       "899996    [92193, 114074, 654526, 931537, 939409, 145880...\n",
       "899997    [7400, 24333, 30210, 60330, 69204, 103918, 103...\n",
       "899998    [71752, 648402, 1251649, 1370659, 1779124, 230...\n",
       "899999    [12838, 12838, 12838, 12838, 122834, 279342, 3...\n",
       "900000    [234063, 285502, 423617, 1224107, 1405713, 180...\n",
       "Name: creative_id, Length: 900000, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<900000x2481135 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 27608868 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from scipy import sparse\n",
    "train_merged=pd.read_csv('train_merged.csv')\n",
    "df=train_merged\n",
    "v = TfidfVectorizer(lowercase=False,tokenizer=lambda x:x)\n",
    "cur=df.groupby(\"user_id\")[\"creative_id\"].apply(list)\n",
    "# cur.astype(int)\n",
    "display(cur)\n",
    "v_fit = v.fit_transform(cur)\n",
    "display(v_fit)\n",
    "sparse.save_npz('v_fit.npz', v_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899995</th>\n",
       "      <td>899996</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899996</th>\n",
       "      <td>899997</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899997</th>\n",
       "      <td>899998</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899998</th>\n",
       "      <td>899999</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899999</th>\n",
       "      <td>900000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  age  gender\n",
       "0             1    4       0\n",
       "1             2   10       0\n",
       "2             3    7       1\n",
       "3             4    5       0\n",
       "4             5    4       0\n",
       "...         ...  ...     ...\n",
       "899995   899996    5       0\n",
       "899996   899997    3       1\n",
       "899997   899998    4       1\n",
       "899998   899999    3       0\n",
       "899999   900000    3       1\n",
       "\n",
       "[900000 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>click_times_mean</th>\n",
       "      <th>advertiser_id_nunique</th>\n",
       "      <th>advertiser_id_count</th>\n",
       "      <th>industry_nunique</th>\n",
       "      <th>industry_count</th>\n",
       "      <th>time_nunique</th>\n",
       "      <th>time_count</th>\n",
       "      <th>product_category_nunique</th>\n",
       "      <th>product_category_count</th>\n",
       "      <th>product_id_nunique</th>\n",
       "      <th>product_id_count</th>\n",
       "      <th>ad_id_nunique</th>\n",
       "      <th>ad_id_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.076923</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.022222</td>\n",
       "      <td>36</td>\n",
       "      <td>45</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>28</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>20</td>\n",
       "      <td>45</td>\n",
       "      <td>42</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>28</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>26</td>\n",
       "      <td>29</td>\n",
       "      <td>10</td>\n",
       "      <td>29</td>\n",
       "      <td>15</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>18</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.030303</td>\n",
       "      <td>30</td>\n",
       "      <td>33</td>\n",
       "      <td>18</td>\n",
       "      <td>33</td>\n",
       "      <td>26</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899995</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899996</th>\n",
       "      <td>1.111111</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899997</th>\n",
       "      <td>1.071429</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899998</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>16</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899999</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        click_times_mean  advertiser_id_nunique  advertiser_id_count  \\\n",
       "0               1.076923                     12                   13   \n",
       "1               1.022222                     36                   45   \n",
       "2               1.000000                     28                   30   \n",
       "3               1.000000                     26                   29   \n",
       "4               1.030303                     30                   33   \n",
       "...                  ...                    ...                  ...   \n",
       "899995          1.000000                     12                   14   \n",
       "899996          1.111111                     13                   18   \n",
       "899997          1.071429                      9                   14   \n",
       "899998          1.000000                     16                   22   \n",
       "899999          1.000000                     10                   12   \n",
       "\n",
       "        industry_nunique  industry_count  time_nunique  time_count  \\\n",
       "0                      9              13            10          13   \n",
       "1                     15              45            28          45   \n",
       "2                      8              30            23          30   \n",
       "3                     10              29            15          29   \n",
       "4                     18              33            26          33   \n",
       "...                  ...             ...           ...         ...   \n",
       "899995                 5              14            12          14   \n",
       "899996                10              18            14          18   \n",
       "899997                 5              14            10          14   \n",
       "899998                14              22            17          22   \n",
       "899999                10              12            12          12   \n",
       "\n",
       "        product_category_nunique  product_category_count  product_id_nunique  \\\n",
       "0                              3                      13                   6   \n",
       "1                              3                      45                  20   \n",
       "2                              6                      30                  17   \n",
       "3                              6                      29                  18   \n",
       "4                              4                      33                   7   \n",
       "...                          ...                     ...                 ...   \n",
       "899995                         3                      14                   5   \n",
       "899996                         4                      18                  10   \n",
       "899997                         4                      14                   5   \n",
       "899998                         7                      22                   5   \n",
       "899999                         2                      12                   2   \n",
       "\n",
       "        product_id_count  ad_id_nunique  ad_id_count  \n",
       "0                     13             12           13  \n",
       "1                     45             42           45  \n",
       "2                     30             30           30  \n",
       "3                     29             29           29  \n",
       "4                     33             33           33  \n",
       "...                  ...            ...          ...  \n",
       "899995                14             13           14  \n",
       "899996                18             17           18  \n",
       "899997                14             14           14  \n",
       "899998                22             18           22  \n",
       "899999                12             12           12  \n",
       "\n",
       "[900000 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reset -f\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "user_info=pd.read_csv('user_info.csv')\n",
    "user_info.astype(int)\n",
    "# display(user_info)\n",
    "agg=pd.read_csv('agg.csv')\n",
    "# agg.astype(int)\n",
    "# display(agg)\n",
    "v_fit=sparse.load_npz('v_fit.npz')\n",
    "num_train=int(user_info.shape[0]*0.8)\n",
    "y_train=user_info[\"gender\"].iloc[:num_train]\n",
    "y_test=user_info[\"gender\"].iloc[num_train:]\n",
    "\n",
    "# X_train=sparse.csr_matrix(v_fit[:num_train,:],dtype=float).tocsr()\n",
    "# X_test=sparse.csr_matrix(v_fit[num_train:,:],dtype=float).tocsr()\n",
    "\n",
    "X_train=sparse.csr_matrix(agg.iloc[:num_train])\n",
    "X_test=sparse.csr_matrix(agg.iloc[num_train:])\n",
    "X_train=sparse.hstack((v_fit[:num_train,:],X_train),dtype=float).tocsr()\n",
    "X_test=sparse.hstack((v_fit[num_train:,:],X_test),dtype=float).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Dataset at 0x7f043c226278>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "# display(X_train,y_train)\n",
    "# display(X_test,y_test)\n",
    "train_data = lgb.Dataset(X_train,y_train)\n",
    "train_data.save_binary('CountVectorizer_train_gender.bin')\n",
    "# test_data=lgb.Dataset(X_test,y_test)\n",
    "# test_data.save_binary('CountVectorizer_test_gender.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "[10]\tcv_agg's auc: 0.6231 + 0.00142673\n",
      "[20]\tcv_agg's auc: 0.628205 + 0.00127108\n",
      "[30]\tcv_agg's auc: 0.641034 + 0.000831938\n",
      "[40]\tcv_agg's auc: 0.648032 + 0.000781777\n",
      "[50]\tcv_agg's auc: 0.656094 + 0.00104904\n",
      "[60]\tcv_agg's auc: 0.66377 + 0.00129256\n",
      "[70]\tcv_agg's auc: 0.668926 + 0.00125677\n",
      "[80]\tcv_agg's auc: 0.673008 + 0.00128241\n",
      "[90]\tcv_agg's auc: 0.676582 + 0.00132133\n",
      "[100]\tcv_agg's auc: 0.679915 + 0.00122345\n",
      "[110]\tcv_agg's auc: 0.683032 + 0.00119218\n",
      "[120]\tcv_agg's auc: 0.685907 + 0.00122485\n",
      "[130]\tcv_agg's auc: 0.688598 + 0.00119378\n",
      "[140]\tcv_agg's auc: 0.691029 + 0.00108103\n",
      "[150]\tcv_agg's auc: 0.693345 + 0.00114\n",
      "[160]\tcv_agg's auc: 0.695466 + 0.00114003\n",
      "[170]\tcv_agg's auc: 0.697398 + 0.0011217\n",
      "[180]\tcv_agg's auc: 0.699246 + 0.00114483\n",
      "[190]\tcv_agg's auc: 0.700961 + 0.00117523\n",
      "[200]\tcv_agg's auc: 0.702576 + 0.00114424\n",
      "[210]\tcv_agg's auc: 0.704087 + 0.00115133\n",
      "[220]\tcv_agg's auc: 0.705509 + 0.00111808\n",
      "[230]\tcv_agg's auc: 0.70679 + 0.00110817\n",
      "[240]\tcv_agg's auc: 0.708069 + 0.00113421\n",
      "[250]\tcv_agg's auc: 0.709261 + 0.00111605\n",
      "[260]\tcv_agg's auc: 0.710449 + 0.00111743\n",
      "[270]\tcv_agg's auc: 0.711508 + 0.00112197\n",
      "[280]\tcv_agg's auc: 0.712499 + 0.00115736\n",
      "[290]\tcv_agg's auc: 0.713464 + 0.00112877\n",
      "[300]\tcv_agg's auc: 0.714413 + 0.00112166\n",
      "[310]\tcv_agg's auc: 0.715273 + 0.00108657\n",
      "[320]\tcv_agg's auc: 0.716076 + 0.0011053\n",
      "[330]\tcv_agg's auc: 0.716876 + 0.00113642\n",
      "[340]\tcv_agg's auc: 0.717626 + 0.00115739\n",
      "[350]\tcv_agg's auc: 0.718368 + 0.00113882\n",
      "[360]\tcv_agg's auc: 0.719089 + 0.00115252\n",
      "[370]\tcv_agg's auc: 0.719761 + 0.00113061\n",
      "[380]\tcv_agg's auc: 0.720371 + 0.00109986\n",
      "[390]\tcv_agg's auc: 0.72097 + 0.00109245\n",
      "[400]\tcv_agg's auc: 0.721553 + 0.00108134\n",
      "[410]\tcv_agg's auc: 0.722098 + 0.0010714\n",
      "[420]\tcv_agg's auc: 0.722661 + 0.00108095\n",
      "[430]\tcv_agg's auc: 0.723187 + 0.00105067\n",
      "[440]\tcv_agg's auc: 0.723656 + 0.00106402\n",
      "[450]\tcv_agg's auc: 0.724121 + 0.00107972\n",
      "[460]\tcv_agg's auc: 0.724569 + 0.00107992\n",
      "[470]\tcv_agg's auc: 0.725008 + 0.00107789\n",
      "[480]\tcv_agg's auc: 0.725393 + 0.00106395\n",
      "[490]\tcv_agg's auc: 0.725793 + 0.0010608\n",
      "[500]\tcv_agg's auc: 0.726203 + 0.00106687\n",
      "[510]\tcv_agg's auc: 0.726566 + 0.00106415\n",
      "[520]\tcv_agg's auc: 0.726961 + 0.00104934\n",
      "[530]\tcv_agg's auc: 0.727292 + 0.00104184\n",
      "[540]\tcv_agg's auc: 0.727613 + 0.00102612\n",
      "[550]\tcv_agg's auc: 0.727953 + 0.00101074\n",
      "[560]\tcv_agg's auc: 0.728265 + 0.000998844\n",
      "[570]\tcv_agg's auc: 0.728582 + 0.000987311\n",
      "[580]\tcv_agg's auc: 0.728839 + 0.00100897\n",
      "[590]\tcv_agg's auc: 0.72913 + 0.00102179\n",
      "[600]\tcv_agg's auc: 0.729388 + 0.0010232\n",
      "[610]\tcv_agg's auc: 0.729621 + 0.00102895\n",
      "[620]\tcv_agg's auc: 0.729877 + 0.00104247\n",
      "[630]\tcv_agg's auc: 0.730144 + 0.000998524\n",
      "[640]\tcv_agg's auc: 0.730376 + 0.00101752\n",
      "[650]\tcv_agg's auc: 0.730606 + 0.00103161\n",
      "[660]\tcv_agg's auc: 0.730817 + 0.00103626\n",
      "[670]\tcv_agg's auc: 0.73103 + 0.00105197\n",
      "[680]\tcv_agg's auc: 0.731258 + 0.00102809\n",
      "[690]\tcv_agg's auc: 0.731478 + 0.00102397\n",
      "[700]\tcv_agg's auc: 0.731634 + 0.001029\n",
      "[710]\tcv_agg's auc: 0.731812 + 0.0010091\n",
      "[720]\tcv_agg's auc: 0.731998 + 0.00104143\n",
      "[730]\tcv_agg's auc: 0.732169 + 0.00102877\n",
      "[740]\tcv_agg's auc: 0.732335 + 0.0010095\n",
      "[750]\tcv_agg's auc: 0.732511 + 0.00101478\n",
      "[760]\tcv_agg's auc: 0.732678 + 0.00101583\n",
      "[770]\tcv_agg's auc: 0.732827 + 0.00102082\n",
      "[780]\tcv_agg's auc: 0.732989 + 0.00101548\n",
      "[790]\tcv_agg's auc: 0.733127 + 0.00101715\n",
      "[800]\tcv_agg's auc: 0.733266 + 0.00101533\n",
      "[810]\tcv_agg's auc: 0.733405 + 0.00101752\n",
      "[820]\tcv_agg's auc: 0.733565 + 0.00101602\n",
      "[830]\tcv_agg's auc: 0.733699 + 0.00101024\n",
      "[840]\tcv_agg's auc: 0.733819 + 0.000991823\n",
      "[850]\tcv_agg's auc: 0.733956 + 0.000960646\n",
      "[860]\tcv_agg's auc: 0.734081 + 0.000964036\n",
      "[870]\tcv_agg's auc: 0.734178 + 0.000969335\n",
      "[880]\tcv_agg's auc: 0.73428 + 0.000978588\n",
      "[890]\tcv_agg's auc: 0.734376 + 0.000987278\n",
      "[900]\tcv_agg's auc: 0.734464 + 0.000972646\n",
      "[910]\tcv_agg's auc: 0.734563 + 0.000970275\n",
      "[920]\tcv_agg's auc: 0.734668 + 0.000970851\n",
      "[930]\tcv_agg's auc: 0.734754 + 0.000973314\n",
      "[940]\tcv_agg's auc: 0.734852 + 0.000963924\n",
      "[950]\tcv_agg's auc: 0.734926 + 0.000968579\n",
      "[960]\tcv_agg's auc: 0.735012 + 0.000967867\n",
      "[970]\tcv_agg's auc: 0.735074 + 0.000977992\n",
      "[980]\tcv_agg's auc: 0.735139 + 0.000965946\n",
      "[990]\tcv_agg's auc: 0.735223 + 0.000978253\n",
      "[1000]\tcv_agg's auc: 0.735306 + 0.000986884\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import lightgbm as lgb\n",
    "train_data = lgb.Dataset('CountVectorizer_train_gender.bin')\n",
    "lgb_params = {'num_leaves': 2**7-1,\n",
    "              'min_data_in_leaf': 25, \n",
    "              'objective':'binary',\n",
    "              'max_depth': -1,\n",
    "              'learning_rate': 0.1,\n",
    "              'boosting': 'gbdt',\n",
    "              'feature_fraction': 0.6,\n",
    "              'bagging_fraction': 0.9,\n",
    "              'bagging_seed': 11,\n",
    "              'metric': 'auc',\n",
    "              'seed':1024,\n",
    "              'nthread':12,\n",
    "             }\n",
    "print(\"start training\")\n",
    "cv_results = lgb.cv(\n",
    "    lgb_params, train_data, num_boost_round=1000, nfold=5, shuffle=True,\n",
    "    early_stopping_rounds=50, verbose_eval=10, show_stdv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict gender\n",
    "%reset -f\n",
    "import lightgbm as lgb\n",
    "train_data = lgb.Dataset('CountVectorizer_train_gender.bin')\n",
    "\n",
    "lgb_params = {'num_leaves': 2**7-1,\n",
    "              'min_data_in_leaf': 25, \n",
    "              'objective':'binary',\n",
    "              'max_depth': -1,\n",
    "              'learning_rate': 0.1,\n",
    "              'boosting': 'gbdt',\n",
    "              'feature_fraction': 0.6,\n",
    "              'bagging_fraction': 0.9,\n",
    "              'bagging_seed': 11,\n",
    "              'metric': 'auc',\n",
    "              'seed':1024,\n",
    "              'nthread':12,\n",
    "             }\n",
    "print(\"start training\")\n",
    "model = lgb.train(lgb_params, train_data,num_boost_round=200)\n",
    "model.save_model(\"model.txt\")\n",
    "print(\"training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start predicting\n",
      "acc: 0.7273166666666666\n",
      "acc: 0.5921415079214687\n"
     ]
    }
   ],
   "source": [
    "# predict gender\n",
    "%reset -f\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "user_info=pd.read_csv('user_info.csv')\n",
    "user_info.astype(int)\n",
    "# display(user_info)\n",
    "agg=pd.read_csv('agg.csv')\n",
    "# display(agg)\n",
    "v_fit=sparse.load_npz('v_fit.npz')\n",
    "num_train=int(user_info.shape[0]*0.8)\n",
    "y_test=user_info[\"gender\"].iloc[num_train:]\n",
    "# display(y_test)\n",
    "X_test=sparse.csr_matrix(agg.iloc[num_train:])\n",
    "X_test=sparse.hstack((v_fit[num_train:,:],X_test),dtype=float).tocsr()\n",
    "print(\"start predicting\")\n",
    "model=lgb.Booster(model_file='model.txt')\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred=(y_pred>0.5)*1\n",
    "print(\"acc:\",metrics.accuracy_score(y_test,y_pred))\n",
    "print(\"acc:\",metrics.roc_auc_score(y_test,y_pred))\n",
    "# submission=pd.DataFrame(result,columns=[\"gender\"])\n",
    "# submission[\"user_id\"]=[i for i in range(num_train+1,user_info.shape[0]+1)]\n",
    "# display(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "[1]\tcv_agg's l1: 1.66908 + 0.00406433\n",
      "[2]\tcv_agg's l1: 1.66908 + 0.00406146\n",
      "[3]\tcv_agg's l1: 1.66907 + 0.00405881\n",
      "[4]\tcv_agg's l1: 1.66906 + 0.00405848\n",
      "[5]\tcv_agg's l1: 1.66905 + 0.00406362\n",
      "[6]\tcv_agg's l1: 1.66906 + 0.00406845\n",
      "[7]\tcv_agg's l1: 1.66904 + 0.00406697\n",
      "[8]\tcv_agg's l1: 1.66904 + 0.00406428\n",
      "[9]\tcv_agg's l1: 1.66903 + 0.0040562\n",
      "[10]\tcv_agg's l1: 1.66903 + 0.00406892\n",
      "[11]\tcv_agg's l1: 1.66903 + 0.00408569\n",
      "[12]\tcv_agg's l1: 1.66903 + 0.00408316\n",
      "[13]\tcv_agg's l1: 1.66903 + 0.004088\n",
      "[14]\tcv_agg's l1: 1.66903 + 0.00409732\n",
      "[15]\tcv_agg's l1: 1.66902 + 0.00409194\n",
      "[16]\tcv_agg's l1: 1.66903 + 0.00409498\n",
      "[17]\tcv_agg's l1: 1.66904 + 0.0040898\n",
      "[18]\tcv_agg's l1: 1.66904 + 0.00409967\n",
      "[19]\tcv_agg's l1: 1.66904 + 0.00409745\n",
      "[20]\tcv_agg's l1: 1.66903 + 0.00410074\n",
      "[21]\tcv_agg's l1: 1.66903 + 0.00409969\n",
      "[22]\tcv_agg's l1: 1.66902 + 0.0041086\n",
      "[23]\tcv_agg's l1: 1.66902 + 0.00410118\n",
      "[24]\tcv_agg's l1: 1.66902 + 0.00409802\n",
      "[25]\tcv_agg's l1: 1.66901 + 0.00410802\n",
      "[26]\tcv_agg's l1: 1.66901 + 0.00410838\n",
      "[27]\tcv_agg's l1: 1.66901 + 0.00411625\n",
      "[28]\tcv_agg's l1: 1.66901 + 0.00411473\n",
      "[29]\tcv_agg's l1: 1.669 + 0.00412288\n",
      "[30]\tcv_agg's l1: 1.66899 + 0.00412204\n",
      "[31]\tcv_agg's l1: 1.66899 + 0.00412319\n",
      "[32]\tcv_agg's l1: 1.66898 + 0.00412727\n",
      "[33]\tcv_agg's l1: 1.66898 + 0.00412263\n",
      "[34]\tcv_agg's l1: 1.66899 + 0.00411721\n",
      "[35]\tcv_agg's l1: 1.66899 + 0.00411053\n",
      "[36]\tcv_agg's l1: 1.66899 + 0.00410609\n",
      "[37]\tcv_agg's l1: 1.66899 + 0.00410495\n",
      "[38]\tcv_agg's l1: 1.66899 + 0.00410406\n",
      "[39]\tcv_agg's l1: 1.669 + 0.00410906\n",
      "[40]\tcv_agg's l1: 1.66899 + 0.00410973\n",
      "[41]\tcv_agg's l1: 1.66899 + 0.00410696\n",
      "[42]\tcv_agg's l1: 1.669 + 0.00410443\n",
      "[43]\tcv_agg's l1: 1.669 + 0.00409683\n",
      "[44]\tcv_agg's l1: 1.669 + 0.0041008\n",
      "[45]\tcv_agg's l1: 1.669 + 0.00409911\n",
      "[46]\tcv_agg's l1: 1.66901 + 0.00409376\n",
      "[47]\tcv_agg's l1: 1.66901 + 0.00409011\n",
      "[48]\tcv_agg's l1: 1.66902 + 0.00408847\n",
      "[49]\tcv_agg's l1: 1.66902 + 0.00409574\n",
      "[50]\tcv_agg's l1: 1.66901 + 0.00409737\n",
      "[51]\tcv_agg's l1: 1.66902 + 0.00410199\n",
      "[52]\tcv_agg's l1: 1.66902 + 0.0041034\n",
      "[53]\tcv_agg's l1: 1.66902 + 0.00410285\n",
      "[54]\tcv_agg's l1: 1.66902 + 0.00410281\n",
      "[55]\tcv_agg's l1: 1.66902 + 0.00410394\n",
      "[56]\tcv_agg's l1: 1.66902 + 0.00410839\n",
      "[57]\tcv_agg's l1: 1.66902 + 0.00410819\n",
      "[58]\tcv_agg's l1: 1.66901 + 0.00411155\n",
      "[59]\tcv_agg's l1: 1.66901 + 0.00410568\n",
      "[60]\tcv_agg's l1: 1.66902 + 0.00410405\n",
      "[61]\tcv_agg's l1: 1.66903 + 0.00410163\n",
      "[62]\tcv_agg's l1: 1.66904 + 0.0040997\n",
      "[63]\tcv_agg's l1: 1.66904 + 0.0041006\n",
      "[64]\tcv_agg's l1: 1.66905 + 0.00410424\n",
      "[65]\tcv_agg's l1: 1.66904 + 0.00410452\n",
      "[66]\tcv_agg's l1: 1.66906 + 0.00410829\n",
      "[67]\tcv_agg's l1: 1.66906 + 0.00411695\n",
      "[68]\tcv_agg's l1: 1.66906 + 0.00411299\n",
      "[69]\tcv_agg's l1: 1.66906 + 0.00410994\n",
      "[70]\tcv_agg's l1: 1.66907 + 0.00411143\n",
      "[71]\tcv_agg's l1: 1.66908 + 0.00411454\n",
      "[72]\tcv_agg's l1: 1.66908 + 0.00411176\n",
      "[73]\tcv_agg's l1: 1.66907 + 0.00411122\n",
      "[74]\tcv_agg's l1: 1.66908 + 0.00411059\n",
      "[75]\tcv_agg's l1: 1.66909 + 0.00410705\n",
      "[76]\tcv_agg's l1: 1.66908 + 0.00410656\n",
      "[77]\tcv_agg's l1: 1.66909 + 0.00410633\n",
      "[78]\tcv_agg's l1: 1.66909 + 0.00410246\n",
      "[79]\tcv_agg's l1: 1.6691 + 0.00410095\n",
      "[80]\tcv_agg's l1: 1.6691 + 0.00410368\n",
      "[81]\tcv_agg's l1: 1.66911 + 0.00410403\n"
     ]
    }
   ],
   "source": [
    "# cv for age\n",
    "%reset -f\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "user_info=pd.read_csv('user_info.csv')\n",
    "user_info.astype(int)\n",
    "agg=pd.read_csv('agg.csv')\n",
    "agg.astype(int)\n",
    "v_fit=sparse.load_npz('v_fit.npz')\n",
    "num_train=int(user_info.shape[0]*0.8)\n",
    "y_train=user_info[\"age\"].iloc[:num_train]\n",
    "y_test=user_info[\"age\"].iloc[num_train:]\n",
    "X_train=sparse.csr_matrix(agg.iloc[:num_train])\n",
    "X_test=sparse.csr_matrix(agg.iloc[num_train:])\n",
    "X_train=sparse.hstack((v_fit[:num_train,:],X_train),dtype=float).tocsr()\n",
    "X_test=sparse.hstack((v_fit[num_train:,:],X_test),dtype=float).tocsr()\n",
    "train_data = lgb.Dataset(X_train,y_train)\n",
    "\n",
    "lgb_params = {'num_leaves': 2**6-1,\n",
    "              'min_data_in_leaf': 25, \n",
    "              'objective':'regression_l2',\n",
    "              'max_depth': -1,\n",
    "              'learning_rate': 0.1,\n",
    "              'boosting': 'gbdt',\n",
    "              'feature_fraction': 0.6,\n",
    "              'bagging_fraction': 0.9,\n",
    "              'bagging_seed': 11,\n",
    "              'metric': 'mae',\n",
    "              'seed':1024,\n",
    "              'nthread':12,\n",
    "              'lambda_l1': 0.2,\n",
    "             }\n",
    "# with open('lgb.txt', 'a') as f:\n",
    "#     f.write(str(lgb_params))\n",
    "print(\"start training\")\n",
    "cv_results = lgb.cv(\n",
    "    lgb_params, train_data, num_boost_round=1000, nfold=5, stratified=False, shuffle=True,\n",
    "    early_stopping_rounds=50, verbose_eval=1, show_stdv=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9d40cfbb2bb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m cv_results = lgb.cv(\n\u001b[1;32m     21\u001b[0m     \u001b[0mlgb_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratified\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     early_stopping_rounds=50, verbose_eval=10, show_stdv=True, seed=0)\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# print('best num_boost_round:', len(cv_results['auc-mean']))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow2_py3/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mcv\u001b[0;34m(params, train_set, num_boost_round, folds, nfold, stratified, shuffle, metrics, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, fpreproc, verbose_eval, show_stdv, seed, callbacks, eval_train_metric)\u001b[0m\n\u001b[1;32m    563\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    564\u001b[0m         \u001b[0mcvfolds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_agg_cv_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcvfolds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_train_metric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow2_py3/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mhandler_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbooster\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboosters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m                 \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandler_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow2_py3/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36meval_valid\u001b[0;34m(self, feval)\u001b[0m\n\u001b[1;32m   2185\u001b[0m             \u001b[0mList\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2186\u001b[0m         \"\"\"\n\u001b[0;32m-> 2187\u001b[0;31m         return [item for i in range_(1, self.__num_dataset)\n\u001b[0m\u001b[1;32m   2188\u001b[0m                 for item in self.__inner_eval(self.name_valid_sets[i - 1], i, feval)]\n\u001b[1;32m   2189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow2_py3/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2186\u001b[0m         \"\"\"\n\u001b[1;32m   2187\u001b[0m         return [item for i in range_(1, self.__num_dataset)\n\u001b[0;32m-> 2188\u001b[0;31m                 for item in self.__inner_eval(self.name_valid_sets[i - 1], i, feval)]\n\u001b[0m\u001b[1;32m   2189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow2_py3/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__inner_eval\u001b[0;34m(self, data_name, data_idx, feval)\u001b[0m\n\u001b[1;32m   2645\u001b[0m                 \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_out_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2647\u001b[0;31m                 result.ctypes.data_as(ctypes.POINTER(ctypes.c_double))))\n\u001b[0m\u001b[1;32m   2648\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtmp_out_len\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_inner_eval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2649\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Wrong length of eval results\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%capture cap --no-stderr\n",
    "%reset -f\n",
    "import lightgbm as lgb\n",
    "train_data = lgb.Dataset('CountVectorizer_gender.bin')\n",
    "lgb_params = {'num_leaves': 2**6-1,\n",
    "              'min_data_in_leaf': 25, \n",
    "              'objective':'binary',\n",
    "              'max_depth': -1,\n",
    "              'learning_rate': 0.1,\n",
    "              'boosting': 'gbdt',\n",
    "              'feature_fraction': 0.6,\n",
    "              'bagging_fraction': 0.9,\n",
    "              'bagging_seed': 11,\n",
    "              'metric': 'auc',\n",
    "              'seed':1024,\n",
    "              'nthread':10,\n",
    "             }\n",
    "# with open('lgb.txt', 'a') as f:\n",
    "#     f.write(str(lgb_params))\n",
    "print(\"start training\")\n",
    "cv_results = lgb.cv(\n",
    "    lgb_params, train_data, num_boost_round=1000, nfold=5, stratified=False, shuffle=True,\n",
    "    early_stopping_rounds=50, verbose_eval=1, show_stdv=True, seed=0)\n",
    "# print('best num_boost_round:', len(cv_results['auc-mean']))\n",
    "# print('best cv score:', cv_results['auc-mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d20096c96fdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"creative_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"ad_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"product_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"product_category\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"advertiser_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"industry\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"time\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"click_times\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"gender\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"user_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# split train data into 2 parts (cross validation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#random state is a seed value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_merged' is not defined"
     ]
    }
   ],
   "source": [
    "# !pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "features=[\"creative_id\",\"ad_id\",\"product_id\",\"product_category\",\"advertiser_id\",\"industry\",\"time\",\"click_times\"]\n",
    "\n",
    "df=train_merged.drop(columns=[\"gender\",\"user_id\"])\n",
    "# split train data into 2 parts (cross validation)\n",
    "train=df.sample(frac=0.8) #random state is a seed value\n",
    "test=df.drop(train.index)\n",
    "y_train=train.pop(\"age\")\n",
    "X_train=train\n",
    "y_test=test.pop(\"age\")\n",
    "X_test=test\n",
    "# display(X_train,y_train)\n",
    "# display(X_test,y_test)\n",
    "train_data.save_binary('age.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import lightgbm as lgb\n",
    "train_data = lgb.Dataset('age.bin')\n",
    "params = {'num_leaves': 2**7-1,\n",
    "          'min_data_in_leaf': 25, \n",
    "          'objective':'regression_l2',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.1,\n",
    "          'min_child_samples': 20,\n",
    "          'boosting': 'gbdt',\n",
    "          'feature_fraction': 0.6,\n",
    "          'bagging_fraction': 0.9,\n",
    "          'bagging_seed': 11,\n",
    "          'metric': 'mae',\n",
    "          'seed':1024,\n",
    "          'lambda_l1': 0.2,\n",
    "          'nthread':12,\n",
    "         }\n",
    "print(\"start training\")\n",
    "cv_results = lgb.cv(\n",
    "    params, train_data, num_boost_round=1000, nfold=5, stratified=False, shuffle=True,\n",
    "    early_stopping_rounds=50, verbose_eval=10, show_stdv=True, seed=0)\n",
    "print('best num_boost_round:', len(cv_results['mae-mean']))\n",
    "print('best cv score:', cv_results['mae-mean'])\n",
    "# bst = lgb.train(param, train_data, num_round,categorical_feature=features)\n",
    "# bst.save_model('test_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "df=tmpdf\n",
    "features=[\"creative_id\"]\n",
    "v = TfidfVectorizer(max_df=1.0,min_df=1,lowercase=False,tokenizer=lambda x:x)\n",
    "for feature in features:\n",
    "    cur=df.groupby(\"user_id\")[feature].apply(list)\n",
    "    v_fit = v.fit_transform(cur)\n",
    "    word=v.get_feature_names()\n",
    "    display(v_fit)\n",
    "    display(type(v_fit))\n",
    "#     for i in range(len(weight)):\n",
    "#         print(\"-----output feature={}, user_id={} tf-idf-----\".format(feature,i+1))\n",
    "#         for j in range(len(word)):\n",
    "#             print(word[j],weight[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_py3",
   "language": "python",
   "name": "conda_tensorflow2_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
